With the increasing scale of publicly available training data, it has become inevitable that some portion of evaluation data is seen during training, and may provide an undue boost in evaluation performance.
Earlier work (Brown et al. (2020), Wei et al.(2022a), Du et al.(2022) in measuring such dataset contamination considered an example from an evaluation set to be “contaminated” if there existed a collision between a high-order n-gram (generally, n = 13) from the sample and the training data.
This was a deliberately conservative approach in order to produce a “clean” subset of the data with high precision, and is used in open-sourced evaluation libraries (e.g. Gao et al. (2021)).
This approach, however, was unable to detect precisely what proportion of a given sample is contaminated, and didn't take into account how evaluation datasets are constructed.
Furthermore, as noted in Chowdhery et al. (2022), some datasets (such as BoolQ) contain contexts extracted verbatim from the web, but not the question and answer continuation.
As such, highly contaminated samples from these datasets are unlikely to gain an unfair advantage.
The methodology in Chowdhery et al. (2022) further improves on the earlier n-gram collision detection by considering a sample to be contaminated if 70% of all 8-grams can be found at least once in the training data.

The previous methodologies noted above all consider contamination in text space, and don't appear to consider the formatting of prompts used for actual evaluation.
In contrast, we instead match on tokenized input, being careful to pass fully verbalized evaluation samples to the tokenizer.
We also diverge from the previous methodologies by considering contamination from a bottom-up perspective.
We consider a token to be contaminated if it appears in any token n-gram longer than 10 tokens in both the evaluation sample and the training set, and define the contamination percentage of a sample to be the percentage of tokens contaminated.
This allows us to view the benchmark performance of our models on a range of contamination scales, while retaining the ability to test a high-precision clean subset (samples with < 20% contamination) and a high-precision contaminated subset (samples with > 80% contamination).
In order to account for the vagaries of the precise format of verbalized samples, we allow a small "skipgram budget" of four tokens, so that matched spans between an evaluation sample and the training data can differ in at most four positions (we do not allow trailing mismatches, or mismatches in the first 10 tokens).
We identify such 10(+)-skipgrams with suffix arrays implemented using a variation of the library from Lee et al. (2022), modified to work on a PySpark cluster (effectively without random access to disk).
Given the embarrassingly parallel nature of the task, we are able to find all such 10-grams (and their full lengths) in our entire dataset in around seven hours (including time to tokenize), utilizing an estimated 1,500 cores.
As there are many confounding factors at play when determining whether dataset contamination has contributed to evaluation performance (mostly stemming from the fact that "clean" and "dirty" subsets do not necessarily well-estimate the population distribution), we make the following assumption: In the event of dataset contamination contributing to evaluation performance, we expect both the "cleanest" examples to have an overall worse average score than their complement, and the "dirtiest" samples to have an overall better average score than their complement.
It is insufficient evidence for contamination if only one of these were true.
To this end, we define four (non-disjoint) subset types as follows: 

- “Clean” samples, with less than 20% token contamination,
- “Not clean” samples, with greater than (or equal to) 20% token contamination,
- “Not dirty” samples, with less than 80% token contamination,
- “Dirty” samples, with greater than (or equal to) 80% token contamination.


There is an additional confounding factor that we attempt to address directly.
With the given definition of contamination (as well as other definitions mentioned in the literature), there is a possibility that a sample may appear contaminated, by virtue of many tokens appearing in matched sequences found in the training data.
However, the matched sequences might be highly fragmented across the training data, in which case it is very unlikely the model saw the correctly-assembled contaminated sequences during training.
To reduce the chance of this phenomenon, we repeat our analysis with minimum match length L \in {10, 20, 30, 40, 50}.
Since in the limit of $L \rightarrow \infinity$ every sample falls into both the "clean" and "not dirty" (there is no contamination), we report the largest L for each dataset that appeared to benefit from contamination to strike a balance between fragmentation and overall contamination.


For each dataset and each of the above sample subset types, we compute both the mean $\overline{X}$ of the performance metric X and the statistic $Z_n = \frac{(\overline{X} \mu_n)}{\sigma_n}$, where n is the size of the sample subset type, and $\mu_n$ and $\sigma^2_n$ are the mean and variance of the sampling distribution of the performance metric for samples of size n, respectively.
By the Central Limit Theorem, Z_n tends towards a standard normal distribution and so we consider there is sufficient evidence to suggest contamination has affected evaluation performance on a dataset if all four sample subsets have $|Z_n| > 2$.
Results for this analysis can be seen in Table 51.
We observe that only HellaSwag and MMLU-Humanities appear to have been boosted due to contamination in the training data, with the 70B model appearing to have gained a greater benefit than the 7B model, as one might expect.
Furthermore, the impact of this effect on MMLU-Humanities appears to cause a benefit for MMLU-Overall for the 70B model, albeit with only a small delta (-0.9) between the "clean" subset performance and the sampling mean.
No other dataset (for any choice of L) appears to have benefitted from dataset contamination, and we omit results from these datasets for conciseness.

An Open Source Data Contamination Report for Llama Series Models

https://arxiv.org/abs/2310.17589